Analysis: see more data at: https://docs.google.com/spreadsheets/d/1vTTcX9EjVpMA67DmTOZLshke-N7cI5u1lpVs5BJ3vOI/edit#gid=0

Observations:
    1. sequential program in scheme is way slower than C++ sequential to begin with. Althought scheme is a bit easier to 
    write after a lot of practice compared to C++, it's clear that scheme is much slower because it's very hard to optimize
    scheme code. There is definetly a better way to write this scheme code to be more scheme-y. However, instead of making 
    that a change (which would interest me maybe in the summer), I wanted to focus more on the timing of the parallel process
    instead. 

    2. I noticed that I was able to acquire data such that every parallel time entry for Drug Design algorithm was faster than
    the time entires of the sequential version. This was reassuring that there is some parallel threads at work here. I have
    split up the TASKS list in 2 lists, and these two lists are being mapped on two different threads. I've only parallelized 
    a portion of the code (the mapping part of assigning score to each ligand in the list), so the reduce and some other small
    operations are being done sequentially. 

    3. I've noticed that although parallel results seem to be consistently faster, it is the case that it's usually not that
    much faster: less than 30%, rather than the 50% i expected from using two threads (see the Thoughts section below).

    4. Althought this is the case, I noticed that as we increase the number of ligands, the difference between the runtime for
    parallel vs serial is more aparent. 

    5. After working on dd_pFutures.scm, where I use the Futures/touch documentation to implement parallelism in the scheme file, 
    I noted that when it finally compiled, it was slower than the sequential scheme file. So I ended up not timing it.

Thoughts:
    While writing my observation, I was thinking about why some parallel timings are very close to the serial timeing. 
    1. There is some additional recursion in the parallel program that helps with splitting the list of tasks in two,
    and then putting them back together.

    2. when a list of tasks is split in two and the parallel processing begins in the program, if LIST1 has ligands that
    are larger in length than LIST2, then it would take longer for list1 to be completed. The parallel program will then
    just have to wait for LIST1 to be completed even though LIST2 has already been completed (so this is not efficient). 

    Another thing I was confused about: if i'm using two threads for the most extensive part of the code, shouldn't the
    runtime of the code be halved? Especially since a bulk of the runtime is in the mapping, which I've parallelized using
    two threads?

        The main issue to the question that I am posing here is that I am assuming that the ENTIRE program is being running
        in parallel on two threads simultaniously. This would mean that the program runtime should be halved. However, it's 
        clear that I've only parallelized the mapping portion of the code and, although that is a bulk of the code, it's 
        not 100% of it. 

        Additionally, there is a small, thought not insignificant amount of code that has been added to the parallel version 
        of drug design. This is the split helper function that recursively splits a list in two. With increasing sizes of list
        this computation may get heavy. Since this function is not present in the sequential version of the code, it adds more
        to the run time of the parallel version, making the run time of the parallel tests be a lot closer to the sequential.

    Using the Amdahl's Law and the data recorded, I would like to calculate the percent of code that was parallelized:  

    >> AMDAHL's LAW (calculations in spreadsheet)
    OVERALL_SPEEDUP = old_excecution_time / new_excecution_time =  ( 1 / ((1 - P) + (P/N)) )
                    = ( 1 / ((1 - P) + (P/N)) )                      P is percent of code parallelized
  ((1 - P) + (P/N)) = 1 / OVERALL_SPEEDUP                           N is the number of processes/threads
         N - NP + P = N / OVERALL_SPEEDUP
          P (1 - N) = (N / OVERALL_SPEEDUP) - N
                  P = ((N / OVERALL_SPEEDUP) - N) / (1 - N)

    Using the AMDAHL's LAW, and our data collection, I would like to see what the law says is the percent of code that we 
    have parallized (solving for variable P). See the spreadsheet for the calculations. 

    The resulting value: 23.01% of the code was parallelized.

    This seems off because the bulk of the code has been parallelized. The most intensive part of the code is the mapping 
    portion that can have worse case (O) = n^3, where n is the lenght of the ligand. It doesn't make sense here to see that
    only 23% of the code was parallelized. 
    
    Some cause of this may be, as previously mentioned in point 2 of this section, is that the uneven lists are waiting for
    each other to be computed and thus, the program waits for both the processes to finish their work before combining their
    output. This makes a lot of sense, though its still concerning that it's happening so often as to reduce the percent so
    low from what I expected. A way of fixing this would be that instead of randomizing the lenght of each ligand, I keep it
    constant. I think it would be very interesting to test that out and see if it makes any changes to the runtime. 


Presetation slides: https://docs.google.com/presentation/d/1vmdLWFapsJwq-ldD9Xce0x__LMxxhwMqkxUoGVayCu4/edit?usp=sharing
redoing time analysis with time.scm implemented below --- 

/*
    **** IMPORTANT NOTE: 
    A lot of the timeigs were recorded in a spreadsheet linked in line 1:
    https://docs.google.com/spreadsheets/d/1vTTcX9EjVpMA67DmTOZLshke-N7cI5u1lpVs5BJ3vOI/edit#gid=0
    Some trials are not listed here because it was easier to record them and
    it looks more presetable on the spreadsheet! 
*/
-------------------------

num ligands = 25
ligands len = 5
                t1  t2  t3
dd-t-serial     25  25  27
dd-t-parallel   22  22  21
-------------------------

num ligands = 50
ligands len = 5
                t1  t2  t3
dd-t-serial     49  58  54
dd-t-parallel   46  54  50
-------------------------

num ligands = 75
ligands len = 5
                t1  t2  t3
dd-t-serial     102 110 100
dd-t-parallel   99  98  96
-------------------------

num ligands = 100
ligands len = 5
                t1  t2  t3
dd-t-serial     172 170 159
dd-t-parallel   154 163 158
-------------------------
-------------------------
-------------------------

num ligands = 25
ligands len = 6
                t1  t2  t3
dd-t-serial     259 258 246
dd-t-parallel   238 229 239
-------------------------

num ligands = 50
ligands len = 6
                t1  t2  t3
dd-t-serial     798 428 811
dd-t-parallel   605 411 635
-------------------------
-------------------------
-------------------------

num ligands = 5
ligands len = 7
                t1  t2  t3
dd-t-serial     (no data: too slow)
dd-t-parallel   
-------------------------

num ligands = 10
ligands len = 7
                t1  t2  t3
dd-t-serial     (no data: too slow)
dd-t-parallel   
-------------------------
/*
    IMPORTANT NOTE: 
    A lot of the timeigs were recorded in a spreadsheet linked in line 1:
    https://docs.google.com/spreadsheets/d/1vTTcX9EjVpMA67DmTOZLshke-N7cI5u1lpVs5BJ3vOI/edit#gid=0
    Some trials are not listed here because it was easier to record them and
    it looks more presetable on the spreadsheet! 
*/